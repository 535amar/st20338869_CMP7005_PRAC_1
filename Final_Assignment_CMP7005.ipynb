{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBJ3MpQMqGQV4UdiELnm0C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/535amar/CMP-7005-Final-Assignment/blob/main/Final_Assignment_CMP7005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CMP7005-PRAC1 Indian Air Quality**\n"
      ],
      "metadata": {
        "id": "Q8t6thHAqHNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfW7Y6bhqJ5A",
        "outputId": "26c0b60b-7c7c-45da-dd2c-27bd16492906"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Colab/Programming for Data Analysis/CMP 7005 Air quality'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqoDJrzuqe6v",
        "outputId": "04cbf059-4d66-4c4c-ad4b-1bf791cc9769"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab/Programming for Data Analysis/CMP 7005 Air quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awZRkPEfrBhT",
        "outputId": "1ea6fc69-e15b-43f4-9f9d-c3da86ec2d89"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ahmedabad_data.csv       Chennai_data.csv     Kochi_data.csv\n",
            "Aizawl_data.csv          Coimbatore_data.csv  Kolkata_data.csv\n",
            "all_cities_combined.csv  Delhi_data.csv       Lucknow_data.csv\n",
            "Amaravati_data.csv       Ernakulam_data.csv   Mumbai_data.csv\n",
            "Amritsar_data.csv        Gurugram_data.csv    Patna_data.csv\n",
            "Bengaluru_data.csv       Guwahati_data.csv    Shillong_data.csv\n",
            "Bhopal_data.csv          Hyderabad_data.csv   Talcher_data.csv\n",
            "Brajrajnagar_data.csv    Jaipur_data.csv      Thiruvananthapuram_data.csv\n",
            "Chandigarh_data.csv      Jorapokhar_data.csv  Visakhapatnam_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# For nicer Display\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 120)\n"
      ],
      "metadata": {
        "id": "g2jvC5h4rHXE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-1 Data handling"
      ],
      "metadata": {
        "id": "lmNioeXCvGlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_files = glob.glob(\"/content/drive/MyDrive/Colab/Programming for Data Analysis/CMP 7005 Air quality/*_data.csv\")\n",
        "print(\"Number of CSV files found:\", len(csv_files))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPID-bkNvFnH",
        "outputId": "eaedc4ee-68e0-4c2e-c899-3d1b786e0bfa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CSV files found: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting file list\n",
        "csv_files = sorted(csv_files)\n",
        "print(\"Files found:\")\n",
        "for f in csv_files:\n",
        "    print(\"  -\", os.path.basename(f))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCmxdarCw4XS",
        "outputId": "8196f3ba-19ab-4b05-8324-c0a62ac6ffe0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files found:\n",
            "  - Ahmedabad_data.csv\n",
            "  - Aizawl_data.csv\n",
            "  - Amaravati_data.csv\n",
            "  - Amritsar_data.csv\n",
            "  - Bengaluru_data.csv\n",
            "  - Bhopal_data.csv\n",
            "  - Brajrajnagar_data.csv\n",
            "  - Chandigarh_data.csv\n",
            "  - Chennai_data.csv\n",
            "  - Coimbatore_data.csv\n",
            "  - Delhi_data.csv\n",
            "  - Ernakulam_data.csv\n",
            "  - Gurugram_data.csv\n",
            "  - Guwahati_data.csv\n",
            "  - Hyderabad_data.csv\n",
            "  - Jaipur_data.csv\n",
            "  - Jorapokhar_data.csv\n",
            "  - Kochi_data.csv\n",
            "  - Kolkata_data.csv\n",
            "  - Lucknow_data.csv\n",
            "  - Mumbai_data.csv\n",
            "  - Patna_data.csv\n",
            "  - Shillong_data.csv\n",
            "  - Talcher_data.csv\n",
            "  - Thiruvananthapuram_data.csv\n",
            "  - Visakhapatnam_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Colab/Programming for Data Analysis/CMP 7005 Air quality/*.csv\"\n",
        "\n",
        "# Find all CSV files in the folder\n",
        "csv_files = [f for f in glob.glob(data_path) if \"all_cities_combined\" not in f]\n",
        "\n",
        "print(\"Number of CSV files found:\", len(csv_files))\n",
        "\n",
        "csv_files = sorted(csv_files)\n",
        "print(\"\\nFiles found:\")\n",
        "for f in csv_files:\n",
        "    print(\"  -\", os.path.basename(f))\n",
        "\n",
        "# Merge all datasets\n",
        "df_list = []\n",
        "\n",
        "for file_path in csv_files:\n",
        "    file_name = os.path.basename(file_path)\n",
        "    city_name = file_name.replace(\"_data.csv\", \"\")\n",
        "\n",
        "    print(f\"\\nLoading: {city_name}\")\n",
        "\n",
        "    df_city = pd.read_csv(file_path)\n",
        "    df_city[\"City\"] = city_name\n",
        "\n",
        "    df_list.append(df_city)\n",
        "\n",
        "all_data = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "print(\"\\n Merged dataset created successfully!\")\n",
        "print(\"Shape:\", all_data.shape)\n",
        "\n",
        "# Save merged dataset back to Drive\n",
        "merged_save_path = \"/content/drive/MyDrive/Colab/Programming for Data Analysis/CMP 7005 Air quality/all_cities_merged.csv\"\n",
        "all_data.to_csv(merged_save_path, index=False)\n",
        "\n",
        "print(\"\\nMerged file saved to:\", merged_save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMmewKBl7hxW",
        "outputId": "47d7c486-754f-4379-e784-86c3e9091e62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CSV files found: 27\n",
            "\n",
            "Files found:\n",
            "  - Ahmedabad_data.csv\n",
            "  - Aizawl_data.csv\n",
            "  - Amaravati_data.csv\n",
            "  - Amritsar_data.csv\n",
            "  - Bengaluru_data.csv\n",
            "  - Bhopal_data.csv\n",
            "  - Brajrajnagar_data.csv\n",
            "  - Chandigarh_data.csv\n",
            "  - Chennai_data.csv\n",
            "  - Coimbatore_data.csv\n",
            "  - Delhi_data.csv\n",
            "  - Ernakulam_data.csv\n",
            "  - Gurugram_data.csv\n",
            "  - Guwahati_data.csv\n",
            "  - Hyderabad_data.csv\n",
            "  - Jaipur_data.csv\n",
            "  - Jorapokhar_data.csv\n",
            "  - Kochi_data.csv\n",
            "  - Kolkata_data.csv\n",
            "  - Lucknow_data.csv\n",
            "  - Mumbai_data.csv\n",
            "  - Patna_data.csv\n",
            "  - Shillong_data.csv\n",
            "  - Talcher_data.csv\n",
            "  - Thiruvananthapuram_data.csv\n",
            "  - Visakhapatnam_data.csv\n",
            "  - all_cities_merged.csv\n",
            "\n",
            "Loading: Ahmedabad\n",
            "\n",
            "Loading: Aizawl\n",
            "\n",
            "Loading: Amaravati\n",
            "\n",
            "Loading: Amritsar\n",
            "\n",
            "Loading: Bengaluru\n",
            "\n",
            "Loading: Bhopal\n",
            "\n",
            "Loading: Brajrajnagar\n",
            "\n",
            "Loading: Chandigarh\n",
            "\n",
            "Loading: Chennai\n",
            "\n",
            "Loading: Coimbatore\n",
            "\n",
            "Loading: Delhi\n",
            "\n",
            "Loading: Ernakulam\n",
            "\n",
            "Loading: Gurugram\n",
            "\n",
            "Loading: Guwahati\n",
            "\n",
            "Loading: Hyderabad\n",
            "\n",
            "Loading: Jaipur\n",
            "\n",
            "Loading: Jorapokhar\n",
            "\n",
            "Loading: Kochi\n",
            "\n",
            "Loading: Kolkata\n",
            "\n",
            "Loading: Lucknow\n",
            "\n",
            "Loading: Mumbai\n",
            "\n",
            "Loading: Patna\n",
            "\n",
            "Loading: Shillong\n",
            "\n",
            "Loading: Talcher\n",
            "\n",
            "Loading: Thiruvananthapuram\n",
            "\n",
            "Loading: Visakhapatnam\n",
            "\n",
            "Loading: all_cities_merged.csv\n",
            "\n",
            " Merged dataset created successfully!\n",
            "Shape: (88593, 16)\n",
            "\n",
            "Merged file saved to: /content/drive/MyDrive/Colab/Programming for Data Analysis/CMP 7005 Air quality/all_cities_merged.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Data Handling Review**\n",
        "\n",
        "In this step, I combined multiple city-wise air quality datasets into a single master DataFrame. Each CSV file represents measurements from one Indian city and follows a similar column structure.\n",
        "\n",
        "To avoid hard-coding file names, I used the `glob` library to automatically detect all files ending with `_data.csv` in the working directory. For each file, I extracted the city name from the filename (e.g. `Ahmedabad_data.csv` → `Ahmedabad`) and stored it in a new `City` column. I then loaded each CSV into a separate pandas DataFrame and concatenated all of them into one dataset called `all_data`.\n",
        "\n",
        "This merged dataset contains air quality measurements for multiple cities across India and will be used as the basis for all further exploratory analysis and modelling.\n"
      ],
      "metadata": {
        "id": "gNq6NaYCNEoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundamental Data Understanding\n",
        "**bold text**\n",
        "After constructing the merged dataset, I performed an initial exploration to understand its structure and quality. I inspected the first few rows, checked the overall shape (number of rows and columns), and listed all column names. I also examined data types and non-null counts to identify which variables are numerical and which contain missing values.\n",
        "\n",
        "Using `describe()`, I generated summary statistics for the numerical features to get a sense of their typical ranges and variability. I then calculated the number of missing values per column and counted duplicate rows. Finally, I verified that the `City` column correctly lists all the different cities present in the dataset, and I converted the `Date` column to a proper datetime format.\n",
        "\n",
        "This fundamental understanding step helps to reveal potential data quality issues (such as missing or inconsistent values) and confirms that the merging process across multiple city files was successful.\n"
      ],
      "metadata": {
        "id": "vGJhUd6INhTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding missing values per column\n",
        "print(all_data.isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujE5zX1xOit-",
        "outputId": "4f84e4d0-f0ca-4858-d377-830b47a4e7fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "City              0\n",
            "Date              0\n",
            "PM2.5         13794\n",
            "PM10          33420\n",
            "NO            10746\n",
            "NO2           10755\n",
            "NOx           12555\n",
            "NH3           30984\n",
            "CO             6177\n",
            "SO2           11562\n",
            "O3            12066\n",
            "Benzene       16869\n",
            "Toluene       24123\n",
            "Xylene        54327\n",
            "AQI           14043\n",
            "AQI_Bucket    14043\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "→ Key Observations:\n",
        "- *Xylene* has extremely high missing values around92%, its not usabale, so should be droped\n",
        "- *PM10, NH3, Toluene* also have the large gaps but still usable after imputation\n",
        "- *AQI* and *AQI_Bucket* missing together\n",
        "- City and Data are complete"
      ],
      "metadata": {
        "id": "niQpXEjbOiBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 Data Pre-processing\n",
        "clean_data = all_data.copy()\n",
        "\n",
        "# 1. Drop columns with extremely high missing values\n",
        "threshold = 0.80  # drop columns with >80% missing values\n",
        "cols_to_drop = []\n",
        "\n",
        "for col in clean_data.columns:\n",
        "    missing_ratio = clean_data[col].isna().mean()\n",
        "    if missing_ratio > threshold:\n",
        "        cols_to_drop.append(col)\n",
        "\n",
        "print(\"Columns dropped due to excessive missing values:\", cols_to_drop)\n",
        "\n",
        "clean_data.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
        "\n",
        "# 2. Convert Date column to datetime\n",
        "clean_data[\"Date\"] = pd.to_datetime(clean_data[\"Date\"], errors=\"coerce\")\n",
        "\n",
        "# Extract Year & Month for analysis\n",
        "clean_data[\"Year\"] = clean_data[\"Date\"].dt.year\n",
        "clean_data[\"Month\"] = clean_data[\"Date\"].dt.month\n",
        "\n",
        "\n",
        "# 3. Remove duplicate rows\n",
        "duplicates = clean_data.duplicated().sum()\n",
        "print(\"Duplicate rows:\", duplicates)\n",
        "\n",
        "clean_data.drop_duplicates(inplace=True)\n",
        "\n",
        "# 4. City-wise median imputation for pollutants\n",
        "\n",
        "pollutant_columns = [\n",
        "    col for col in clean_data.columns\n",
        "    if col not in [\"City\", \"Date\", \"AQI_Bucket\", \"Year\", \"Month\"]\n",
        "    and clean_data[col].dtype != \"object\"\n",
        "]\n",
        "\n",
        "print(\"Numeric pollutant columns to impute:\", pollutant_columns)\n",
        "\n",
        "# Apply median imputation grouped by city\n",
        "for col in pollutant_columns:\n",
        "    clean_data[col] = clean_data.groupby(\"City\")[col].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "\n",
        "\n",
        "# 5. Fix AQI and AQI_Bucket issues\n",
        "# Simple fill of AQI missing using city median (baseline approach)\n",
        "if \"AQI\" in clean_data.columns:\n",
        "    clean_data[\"AQI\"] = clean_data.groupby(\"City\")[\"AQI\"].transform(\n",
        "        lambda x: x.fillna(x.median())\n",
        "    )\n",
        "\n",
        "# If AQI_Bucket still missing, fill with \"Unknown\"\n",
        "if \"AQI_Bucket\" in clean_data.columns:\n",
        "    clean_data[\"AQI_Bucket\"] = clean_data[\"AQI_Bucket\"].fillna(\"Unknown\")\n",
        "\n",
        "print(\"\\nPre-processing complete!\")\n",
        "print(\"Final dataset shape:\", clean_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCieC93ETdXw",
        "outputId": "bf20dcd7-85c4-47e9-e120-9db8b0527121"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns dropped due to excessive missing values: []\n",
            "Duplicate rows: 32568\n",
            "Numeric pollutant columns to impute: ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI']\n",
            "\n",
            "Pre-processing complete!\n",
            "Final dataset shape: (56025, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-from the all the data set i cleaned and prepared the merged datast. The first issue i found in the data handling step was the presence of missing values across almost all pollutant columns. Some variables such as Xylene contained more than 90% missing values, making them unsutable for analysis or modeling so this column was removed entirely.\n",
        "\n",
        "-For the remaing numerical varible i applied a city wise median imputation strategy. Air quality levels vary signficantly between cities therefore, calculating a global median could bias the dataset. Using a per-city median ensures the imputed values reflect the local pollution characteristics.\n",
        "\n",
        "-Duplicated rows were also checked and removed."
      ],
      "metadata": {
        "id": "44oCk7RTTseU"
      }
    }
  ]
}